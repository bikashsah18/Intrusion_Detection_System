{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Intrusion Detection â€” Inference and Evaluation\n\nThis notebook loads pre-trained models and evaluates them on the KDD test data.\nIt mirrors the structure of the training notebook and is designed to be run top-to-bottom.\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## Setup and Imports\n"}, {"cell_type": "code", "execution_count": null, "id": "83be141a-d4b7-406c-994f-95f3b65ef38e", "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import joblib\n", "import pickle\n", "from tensorflow import keras\n", "from sklearn import metrics\n", "from sklearn.preprocessing import RobustScaler\n", "import matplotlib.pyplot as plt\n", "from sklearn.decomposition import PCA\n", "from sklearn.model_selection import train_test_split\n", "import tensorflow as tf\n", "from tensorflow.keras import regularizers\n", "import warnings\n", "pd.set_option('display.max_columns',None)\n", "warnings.filterwarnings('ignore')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "## Load Test Data\n"}, {"cell_type": "code", "execution_count": null, "id": "0a7c01ae-e128-4002-9f1a-941f97f45135", "metadata": {}, "outputs": [], "source": ["df_test = pd.read_csv(\"../data/raw/KDDTest.txt\")\n", "df_test.info()\n"]}, {"cell_type": "code", "execution_count": null, "id": "cd45c56c-842d-4382-9db8-a898ebc3d0f5", "metadata": {}, "outputs": [], "source": ["columns = (['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent','hot'\n", ",'num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations'\n", ",'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count','serror_rate'\n", ",'srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate','srv_diff_host_rate','dst_host_count','dst_host_srv_count'\n", ",'dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate','dst_host_srv_diff_host_rate','dst_host_serror_rate'\n", ",'dst_host_srv_serror_rate','dst_host_rerror_rate','dst_host_srv_rerror_rate','outcome','level'])\n"]}, {"cell_type": "code", "execution_count": null, "id": "0660c6ca-9699-41d9-a2e5-339a44ed218c", "metadata": {}, "outputs": [], "source": ["df_test.columns = columns\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "## Preprocessing\n"}, {"cell_type": "code", "execution_count": null, "id": "9d119bbb-11b4-4af2-a8a4-e8e1382fd133", "metadata": {}, "outputs": [], "source": ["# Manual mapping for protocol_type\n", "protocol_map = {'tcp': 1, 'udp': 2, 'icmp': 3}\n", "df_test['protocol_type'] = df_test['protocol_type'].map(protocol_map)\n"]}, {"cell_type": "code", "execution_count": null, "id": "c7c8f791-2675-4c1e-9e54-4b23eaedcdbd", "metadata": {}, "outputs": [], "source": ["df_test.info()\n"]}, {"cell_type": "code", "execution_count": null, "id": "422603a8-b3bc-46d5-b57c-96995b8fe7a7", "metadata": {}, "outputs": [], "source": ["df_test.describe().style.background_gradient(cmap='Blues').set_properties(**{'font-family':'Segoe UI'})\n"]}, {"cell_type": "code", "execution_count": null, "id": "e3bef3e8-dc82-4261-ada1-cef5d95df0c0", "metadata": {}, "outputs": [], "source": ["df_test.loc[df_test['outcome'] == \"normal\", \"outcome\"] = 0\n", "df_test.loc[df_test['outcome'] != 0, \"outcome\"] = 1\n"]}, {"cell_type": "code", "execution_count": null, "id": "eb8f1482-0708-4095-af79-58a73eecb23f", "metadata": {}, "outputs": [], "source": ["def pie_plot(df, cols_list, rows, cols):\n", "    fig, axes = plt.subplots(rows, cols)\n", "    for ax, col in zip(axes.ravel(), cols_list):\n", "        df[col].value_counts().plot(ax=ax, kind='pie', figsize=(15, 15), fontsize=10, autopct='%1.0f%%')\n", "        ax.set_title(str(col), fontsize = 12)\n", "    plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "161fc86d-40d3-4353-81d7-1b061c304df1", "metadata": {}, "outputs": [], "source": ["pie_plot(df_test, ['protocol_type', 'outcome'], 1, 2)\n"]}, {"cell_type": "code", "execution_count": null, "id": "ec253283-8ad0-417d-b40f-cce98e3a3a30", "metadata": {}, "outputs": [], "source": ["df_test.head()\n"]}, {"cell_type": "markdown", "id": "f5b6adc5-f428-483e-a605-6b767a3cc9cf", "metadata": {}, "source": ["## DATA PREPRCESSING"]}, {"cell_type": "markdown", "metadata": {}, "source": "### Scaling (RobustScaler)\n"}, {"cell_type": "code", "execution_count": null, "id": "ef1dae1f-e26a-4d8d-9e66-a6213af815b3", "metadata": {}, "outputs": [], "source": ["def preprocess(dataframe, train_columns=None):\n", "    # Numeric columns\n", "    cat_cols = ['is_host_login','protocol_type','service','flag','land', 'logged_in','is_guest_login', 'level', 'outcome']\n", "    df_num = dataframe.drop(cat_cols, axis=1)\n", "    num_cols = df_num.columns\n", "\n", "    # Scale numeric columns\n", "    scaler = RobustScaler()\n", "    scaled_df = scaler.fit_transform(df_num)\n", "    scaled_df = pd.DataFrame(scaled_df, columns=num_cols)\n", "\n", "    # Combine scaled numeric data with categorical\n", "    dataframe.drop(labels=num_cols, axis=\"columns\", inplace=True)\n", "    dataframe[num_cols] = scaled_df\n", "\n", "    # One-hot encode categorical columns\n", "    dataframe = pd.get_dummies(dataframe, columns = ['protocol_type', 'service', 'flag'])\n", "\n", "    # Ensure train and test sets have the same columns\n", "    if train_columns is not None:\n", "        missing_cols = set(train_columns) - set(dataframe.columns)\n", "        for col in missing_cols:\n", "            dataframe[col] = 0\n", "        dataframe = dataframe[train_columns]\n", "\n", "    return dataframe\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "## Load Trained Models (Classical ML)\n"}, {"cell_type": "code", "execution_count": null, "id": "8d9be90c-2743-426c-bdf7-a6b5d945e224", "metadata": {}, "outputs": [], "source": ["model_lr = joblib.load('../artifacts/model_lr.pkl')\n", "train_columns = joblib.load('../artifacts/train_columns.pkl')\n", "train_columns.nunique()\n"]}, {"cell_type": "code", "execution_count": null, "id": "46e64552-f59f-4d0a-8ca2-8b4538f35a31", "metadata": {}, "outputs": [], "source": ["scaled_test = preprocess(df_test, train_columns)\n"]}, {"cell_type": "code", "execution_count": null, "id": "3ac34eb4-bc52-4e69-90b1-c5b149cc1110", "metadata": {}, "outputs": [], "source": ["test_columns = scaled_test.columns\n", "test_columns.nunique()\n"]}, {"cell_type": "code", "execution_count": null, "id": "14d5c8e4-f37f-4702-9b44-f8bfd89acc2b", "metadata": {}, "outputs": [], "source": ["# Find missing columns in test data\n", "missing_in_test = set(train_columns) - set(test_columns)\n", "\n", "# Find extra columns in the test data that aren't in the training data\n", "extra_in_test = set(test_columns) - set(train_columns)\n", "\n", "for col in missing_in_test:\n", "    scaled_test[col] = 0\n", "\n", "# Remove extra columns from test data\n", "scaled_test = scaled_test.drop(columns=extra_in_test, errors='ignore')\n", "\n", "# Ensure the test data columns are in the same order as the train data\n", "scaled_test = scaled_test[train_columns]\n", "\n", "print(\"Test columns after alignment:\", scaled_test.columns)\n"]}, {"cell_type": "code", "execution_count": null, "id": "d653f0a3-6882-42c0-881e-d07e38386e50", "metadata": {}, "outputs": [], "source": ["scaled_test.info()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "## Metrics and Evaluation Helpers\n"}, {"cell_type": "code", "execution_count": null, "id": "8b4b8c61-507c-4bc6-921b-c660a068ee97", "metadata": {}, "outputs": [], "source": ["kernal_evals = dict()\n", "def evaluate_classification(model, name, X_train, X_test, y_train, y_test):\n", "    # Predict class labels (apply thresholding for binary classification)\n", "    train_predictions = (model.predict(X_train) > 0.5).astype(int)\n", "    test_predictions = (model.predict(X_test) > 0.5).astype(int)\n", "\n", "    # Calculate accuracy, precision, and recall\n", "    train_accuracy = metrics.accuracy_score(y_train, train_predictions)\n", "    test_accuracy = metrics.accuracy_score(y_test, test_predictions)\n", "\n", "    train_precision = metrics.precision_score(y_train, train_predictions)\n", "    test_precision = metrics.precision_score(y_test, test_predictions)\n", "\n", "    train_recall = metrics.recall_score(y_train, train_predictions)\n", "    test_recall = metrics.recall_score(y_test, test_predictions)\n", "\n", "    # Store results in the dictionary\n", "    kernal_evals[str(name)] = [train_accuracy, test_accuracy, train_precision, test_precision, train_recall, test_recall]\n", "\n", "    # Print results\n", "    print(f\"Training Accuracy {name}: {train_accuracy * 100:.2f}%  Test Accuracy {name}: {test_accuracy * 100:.2f}%\")\n", "    print(f\"Training Precision {name}: {train_precision * 100:.2f}%  Test Precision {name}: {test_precision * 100:.2f}%\")\n", "    print(f\"Training Recall {name}: {train_recall * 100:.2f}%  Test Recall {name}: {test_recall * 100:.2f}%\")\n", "\n", "    # Confusion matrix\n", "    actual = y_test\n", "    confusion_matrix = metrics.confusion_matrix(actual, test_predictions)\n", "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=['normal', 'attack'])\n", "\n", "    # Plot confusion matrix\n", "    fig, ax = plt.subplots(figsize=(10, 10))\n", "    ax.grid(False)\n", "    cm_display.plot(ax=ax)\n", "    plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "bbead4b0-b486-4817-89f0-edf79430ed38", "metadata": {}, "outputs": [], "source": ["for column in df_test.columns:\n", "    print(column.upper(),':',df_test[column].nunique())\n", "    # print(data_train.value_counts())\n"]}, {"cell_type": "markdown", "id": "40a3e697-b48c-4a9e-89c3-7177f3e6d0a3", "metadata": {}, "source": ["## PCA"]}, {"cell_type": "markdown", "metadata": {}, "source": "### Dimensionality Reduction (PCA)\n"}, {"cell_type": "code", "execution_count": null, "id": "f63ab9ba-0cc9-420a-aa8a-e2b92e08fac8", "metadata": {}, "outputs": [], "source": ["x = scaled_test.drop(['outcome', 'level'], axis=1)\n", "print(x.columns)\n", "x = x.values\n", "y = scaled_test['outcome'].values\n", "y_reg = scaled_test['level'].values\n", "pca = PCA(n_components=20)\n", "pca = pca.fit(x)\n", "x_reduced = pca.transform(x)\n", "print(\"Number of original features is {} and of reduced features is {}\".format(x.shape[1], x_reduced.shape[1]))\n", "y = y.astype('int')\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n", "x_train_reduced, x_test_reduced, y_train_reduced, y_test_reduced = train_test_split(x_reduced, y, test_size=0.2, random_state=42)\n", "x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(x, y_reg, test_size=0.2, random_state=42)\n"]}, {"cell_type": "code", "execution_count": null, "id": "65dff0df-8028-454a-bc8e-e5e09d386743", "metadata": {}, "outputs": [], "source": ["model_lr = joblib.load('../artifacts/model_lr.pkl')\n", "model_knn = joblib.load('../artifacts/model_knn.pkl')\n", "model_gnb = joblib.load('../artifacts/model_gnb.pkl')\n", "model_linear_svc = joblib.load('../artifacts/model_linear_svc.pkl')\n", "model_tdt = joblib.load('../artifacts/model_tdt.pkl')\n", "model_rf = joblib.load('../artifacts/model_rf.pkl')\n", "model_xg_r = joblib.load('../artifacts/model_xg_r.pkl')\n", "model_rrf = joblib.load('../artifacts/model_rrf.pkl')\n"]}, {"cell_type": "code", "execution_count": null, "id": "9b1b3b6e-cfa8-4684-82c0-93a720328710", "metadata": {}, "outputs": [], "source": ["try:\n", "    x_train = x_train.astype(np.float32)\n", "    y_train = y_train.astype(np.int32)  # Use float32 if it's a regression problem\n", "    x_test = x_test.astype(np.float32)\n", "    y_test = y_test.astype(np.int32)\n", "    model_history = joblib.load('../artifacts/model_history.pkl')\n", "    print(\"Model history loaded successfully.\")\n", "except Exception as e:\n", "    print(\"Error loading model history:\", e)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "## Deep Learning (Loaded Model)\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Load pre-trained Deep Learning model from artifacts\n", "import os\n", "import joblib\n", "import numpy as np\n", "import tensorflow as tf\n", "\n", "_dl_model = None\n", "_joblib_path = '../artifacts/model_dl.pkl'\n", "_savedmodel_dir = '../artifacts/model_dl'\n", "_h5_path = '../artifacts/model_dl.h5'\n", "\n", "try:\n", "    if os.path.exists(_joblib_path):\n", "        _dl_model = joblib.load(_joblib_path)\n", "        print('Loaded DL model from', _joblib_path)\n", "    elif os.path.isdir(_savedmodel_dir):\n", "        _dl_model = tf.keras.models.load_model(_savedmodel_dir)\n", "        print('Loaded DL model from', _savedmodel_dir)\n", "    elif os.path.exists(_h5_path):\n", "        _dl_model = tf.keras.models.load_model(_h5_path)\n", "        print('Loaded DL model from', _h5_path)\n", "except Exception as e:\n", "    print('Failed to load DL model:', e)\n", "\n", "if _dl_model is None:\n", "    print('Deep Learning model artifact not found in ../artifacts/. Provide model_dl.pkl or model_dl(.h5).')\n"]}, {"cell_type": "code", "execution_count": null, "id": "5d1147d9-6558-4618-b2b5-bba7d1eb6504", "metadata": {}, "outputs": [], "source": ["x_train = x_train.astype(np.float32)\n", "y_train = y_train.astype(np.int32)  # Use float32 if it's a regression problem\n", "x_test = x_test.astype(np.float32)\n", "y_test = y_test.astype(np.int32)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Evaluate the loaded DL model if available\n", "from sklearn import metrics\n", "if _dl_model is not None:\n", "    x_eval = x_test.astype(np.float32)\n", "    y_eval = y_test.astype(np.int32)\n", "    try:\n", "        y_pred_probs = _dl_model.predict(x_eval)\n", "    except Exception:\n", "        y_pred_probs = _dl_model.predict(x_eval)\n", "    if hasattr(y_pred_probs, 'numpy'):\n", "        y_pred_probs = y_pred_probs.numpy()\n", "    import numpy as _np\n", "    if _np.ndim(y_pred_probs) > 1:\n", "        y_pred_probs = _np.ravel(y_pred_probs)\n", "    y_pred = (y_pred_probs >= 0.5).astype(int)\n", "\n", "    print(f'DL Test Accuracy: {metrics.accuracy_score(y_eval, y_pred) * 100:.2f}%')\n", "    print(f'DL Test Precision: {metrics.precision_score(y_eval, y_pred) * 100:.2f}%')\n", "    print(f'DL Test Recall: {metrics.recall_score(y_eval, y_pred) * 100:.2f}%')\n", "\n", "    cm = metrics.confusion_matrix(y_eval, y_pred)\n", "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['normal', 'attack'])\n", "    disp.plot()\n"]}, {"cell_type": "code", "execution_count": null, "id": "0a3291f0-04d7-40bd-9439-aa7907b1e524", "metadata": {}, "outputs": [], "source": ["plt.plot(model_history.history['loss'], label='loss')\n", "plt.plot(model_history.history['val_loss'], label='val_loss')\n", "plt.xlabel('Epoch')\n", "plt.ylabel('SCCE Loss')\n", "plt.legend()\n", "plt.grid(True)\n"]}, {"cell_type": "code", "execution_count": null, "id": "296b7de0-7f17-49b4-bd13-21a52bfe06cd", "metadata": {}, "outputs": [], "source": ["plt.plot(model_history.history['accuracy'], label='accuracy')\n", "plt.plot(model_history.history['val_accuracy'], label='val_accuracy')\n", "plt.xlabel('Epoch')\n", "plt.ylabel('Accuracy')\n", "plt.legend()\n", "plt.grid(True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "## Evaluate Classical ML Models\n"}, {"cell_type": "code", "execution_count": null, "id": "4a49aac5-03f3-4c1e-98c7-f6dd44197017", "metadata": {}, "outputs": [], "source": ["evaluate_classification(model_rrf, \"RRF\", x_train_reduced, x_test_reduced, y_train_reduced, y_test_reduced)\n"]}, {"cell_type": "code", "execution_count": null, "id": "b911c8ac-aad3-4cc3-b68d-ee12afd6ea8c", "metadata": {}, "outputs": [], "source": ["df_test['protocol_type']\n", "df_test['protocol_type'] = df_test['protocol_type'].astype(float)\n", "print(scaled_test.nunique())\n", "x_train = pd.DataFrame(x_train)\n", "print(x_train.columns)\n"]}, {"cell_type": "code", "execution_count": null, "id": "f770b166-958e-41c3-b3fd-d6750f953abc", "metadata": {}, "outputs": [], "source": ["def convert_to_numeric(df):\n", "    for column in df.columns:\n", "        if df[column].dtype == 'object':\n", "            try:\n", "                df[column] = pd.to_numeric(df[column])\n", "            except ValueError:\n", "                df[column] = df[column].astype('category').cat.codes\n", "    return df\n", "x_train = convert_to_numeric(pd.DataFrame(x_train))\n", "x_test = convert_to_numeric(pd.DataFrame(x_test))\n", "print(\"Evaluating models on the new test data...\")\n"]}, {"cell_type": "code", "execution_count": null, "id": "9ccfb1ba-d3c7-40a0-9870-7c757e77bc5a", "metadata": {}, "outputs": [], "source": ["evaluate_classification(model_lr, \"Logistic Regression\", x_train, x_test, y_train, y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "edb9ae80-c4a7-4e17-9a72-0432b71cb6d5", "metadata": {}, "outputs": [], "source": ["evaluate_classification(model_knn, \"KNN\", x_train, x_test, y_train, y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "1e9de935-966d-4bb2-91f1-e166dfe6a306", "metadata": {}, "outputs": [], "source": ["\n", "evaluate_classification(model_gnb, \"GaussianNB\", x_train, x_test, y_train, y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "6e5e15f0-0fc9-46b6-b2ef-9a2fc29068fb", "metadata": {}, "outputs": [], "source": ["\n", "evaluate_classification(model_tdt, \"Decision Tree\", x_train, x_test, y_train, y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "827031e1-864a-43f2-b9e4-d5301838476a", "metadata": {}, "outputs": [], "source": ["\n", "evaluate_classification(model_rf, \"Random Forest\", x_train, x_test, y_train, y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "2b340222-4143-4190-b4e9-f08860dffbfe", "metadata": {}, "outputs": [], "source": ["\n", "evaluate_classification(model_xg_r, \"XGBoost\", x_train, x_test, y_train, y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "f4ded7ea-8e24-4cf6-873a-934c682ab5fc", "metadata": {}, "outputs": [], "source": ["evaluate_classification(model_linear_svc, \"Linear SVC\", x_train, x_test, y_train, y_test)\n"]}, {"cell_type": "code", "execution_count": null, "id": "ac753a55-c0a9-4c10-bf8e-4b053e9cdc99", "metadata": {}, "outputs": [], "source": ["keys = [key for key in kernal_evals.keys()]\n", "values = [value for value in kernal_evals.values()]\n", "fig, ax = plt.subplots(figsize=(20, 6))\n", "ax.bar(np.arange(len(keys)) - 0.2, [value[0] for value in values], color='darkred', width=0.25, align='center')\n", "ax.bar(np.arange(len(keys)) + 0.2, [value[1] for value in values], color='y', width=0.25, align='center')\n", "ax.legend([\"Training Accuracy\", \"Test Accuracy\"])\n", "ax.set_xticklabels(keys)\n", "ax.set_xticks(np.arange(len(keys)))\n", "plt.ylabel(\"Accuracy\")\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "3a7f02da-8b0d-4f45-81a3-0a28df48df5e", "metadata": {}, "outputs": [], "source": ["keys = [key for key in kernal_evals.keys()]\n", "values = [value for value in kernal_evals.values()]\n", "fig, ax = plt.subplots(figsize=(20, 6))\n", "ax.bar(np.arange(len(keys)) - 0.2, [value[2] for value in values], color='g', width=0.25, align='center')\n", "ax.bar(np.arange(len(keys)) + 0.2, [value[3] for value in values], color='b', width=0.25, align='center')\n", "ax.legend([\"Training Precesion\", \"Test Presision\"])\n", "ax.set_xticklabels(keys)\n", "ax.set_xticks(np.arange(len(keys)))\n", "plt.ylabel(\"Precesion\")\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "3204889f-987f-4cec-acc6-533c5acfa41b", "metadata": {}, "outputs": [], "source": ["keys = [key for key in kernal_evals.keys()]\n", "values = [value for value in kernal_evals.values()]\n", "fig, ax = plt.subplots(figsize=(20, 6))\n", "ax.bar(np.arange(len(keys)) - 0.2, [value[2] for value in values], color='g', width=0.25, align='center')\n", "ax.bar(np.arange(len(keys)) + 0.2, [value[3] for value in values], color='b', width=0.25, align='center')\n", "ax.legend([\"Training Recall\", \"Test Recall\"])\n", "ax.set_xticklabels(keys)\n", "ax.set_xticks(np.arange(len(keys)))\n", "plt.ylabel(\"Recall\")\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "### Deep Learning (loaded model)\nThis section loads a pre-trained neural network from `../artifacts/` instead of training in-notebook."}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Load pre-trained Deep Learning model from artifacts\n", "import os\n", "import joblib\n", "import numpy as np\n", "import tensorflow as tf\n", "\n", "_dl_model = None\n", "_joblib_path = '../artifacts/model_dl.pkl'\n", "_savedmodel_dir = '../artifacts/model_dl'\n", "_h5_path = '../artifacts/model_dl.h5'\n", "\n", "try:\n", "    if os.path.exists(_joblib_path):\n", "        _dl_model = joblib.load(_joblib_path)\n", "        print('Loaded DL model from', _joblib_path)\n", "    elif os.path.isdir(_savedmodel_dir):\n", "        _dl_model = tf.keras.models.load_model(_savedmodel_dir)\n", "        print('Loaded DL model from', _savedmodel_dir)\n", "    elif os.path.exists(_h5_path):\n", "        _dl_model = tf.keras.models.load_model(_h5_path)\n", "        print('Loaded DL model from', _h5_path)\n", "except Exception as e:\n", "    print('Failed to load DL model:', e)\n", "\n", "if _dl_model is None:\n", "    print('Deep Learning model artifact not found. Place model_dl.pkl or model_dl(.h5) under ../artifacts/.')\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Evaluate the loaded DL model if available\n", "from sklearn import metrics\n", "if _dl_model is not None:\n", "    x_eval = x_test.astype(np.float32)\n", "    y_eval = y_test.astype(np.int32)\n", "    y_pred_probs = _dl_model.predict(x_eval)\n", "    import numpy as _np\n", "    if _np.ndim(y_pred_probs) > 1:\n", "        y_pred_probs = _np.ravel(y_pred_probs)\n", "    y_pred = (y_pred_probs >= 0.5).astype(int)\n", "    print(f'DL Test Accuracy: {metrics.accuracy_score(y_eval, y_pred) * 100:.2f}%')\n", "    print(f'DL Test Precision: {metrics.precision_score(y_eval, y_pred) * 100:.2f}%')\n", "    print(f'DL Test Recall: {metrics.recall_score(y_eval, y_pred) * 100:.2f}%')\n", "    cm = metrics.confusion_matrix(y_eval, y_pred)\n", "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['normal', 'attack'])\n", "    disp.plot()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": "## Next Steps\n"}, {"cell_type": "markdown", "metadata": {}, "source": "- Ensure the DL artifact exists at `../artifacts/model_dl.pkl` (or SavedModel/H5).\n- Re-run from the top to reproduce results and refresh plots.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.19"}}, "nbformat": 4, "nbformat_minor": 5}